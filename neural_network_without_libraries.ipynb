{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaSxkADUz81qcV6l8GRjQD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niharikasingh722/Neural-Networks/blob/main/neural_network_without_libraries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "-UMniflAMwGr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(y_true,y_pred):\n",
        "  epsilon=1e-15\n",
        "  y_pred_new=[max(i,epsilon) for i in y_pred]\n",
        "  y_pred_new=[min(i,1-epsilon) for i in y_pred]\n",
        "  y_pred=np.array(y_pred_new)\n",
        "  loss=-np.mean(y_true*np.log(y_pred)+(1-y_true)*np.log(1-y_pred))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "sMa8R5WnP5Nc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ea-QddkHJcDA"
      },
      "outputs": [],
      "source": [
        "class myNN:\n",
        "  def __init__(self):\n",
        "    self.w1=1\n",
        "    self.w2=1\n",
        "    self.bias=0\n",
        "\n",
        "  def gradient_descent(self, age, affordability,y_true,epochs,loss_threshold):\n",
        "    w1,w1=1\n",
        "    bias=0\n",
        "\n",
        "    for i in range(epochs):\n",
        "      wt_sum=w1*age+w2*affordability+bias\n",
        "      y_pred=np.sigmoid(wt_sum)\n",
        "      loss=log_loss(y_true,y_pred)\n",
        "      learning_rate=.01\n",
        "      w1_update=np.dot(np.transpose(age),(y_pred-y_true))\n",
        "      w2_update=np.dot(np.transpose(affordability),(y_pred-y_true))\n",
        "      bias_update=(y_pred-y_true)\n",
        "\n",
        "      w1=w1-learning_rate*np.mean(w1_update)\n",
        "      w2=w2-learning_rate*np.mean(w2_update)\n",
        "      bias=bias-learning_rate*np.mean(bias_update)\n",
        "\n",
        "      print(f'Epoch: {i}, w1={np.round(w1,4)}, w2={np.round(w2,4)}, bias={np.round(bias,4)}')\n",
        "\n",
        "      if loss<=loss_threshold:\n",
        "        break\n",
        "\n",
        "    return w1,w2,bias\n",
        "\n",
        "  def fit(self,X,y_true,epochs,loss_threshold):\n",
        "    self.w1,self.w2,self.bias=self.gradient_descent(X['age'],X['affordability'],y_true,epochs,loss_threshold)\n",
        "\n",
        "  def predict(self,X):\n",
        "    wt_sum=self.w1*X['age']+self.w2*X['affordability']+self.bias\n",
        "    return(np.sigmoid(wt_sum))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}